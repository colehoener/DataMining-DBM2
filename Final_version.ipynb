{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Project DBM2\n",
    "\n",
    "This is a Data Mining Project from Cole Hoener, Brady Coaldale and Lukas Schirren.\n",
    "\n",
    "For our project did we pick the <a href=\"http://www.econ.yale.edu/~shiller/data.htm\" target=\"_blank\">U.S. Stock Markets 1871-Present and CAPE Ratio</a> data from Robert Shiller. \n",
    "The data set consists of monthly stock price, dividends, and earnings data and the consumer price index. It also includes the Cyclically adjusted price-to-earnings (CAPE) ratio, which is used to analyze a company's long-term financial performance. An extremly high value could signal that a company or stock is over-valued (vice versa for low values). In the past, the CAPE ratio could identify potential bubbles or market crashes. \n",
    "\n",
    "We start with the data exploration and then do the preprocessing of the data. After that, we continue with classification and end with cluster analysis.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "The data has 20 columns, from which 15 columns are calcuations performed on the first 5 columns. The first column describes the year and month (YYYY-MM). The following 4 columns are 's_and_p_comp', 'dividend', 'earnings' and 'CPI' (Consumer Price Index).\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "Let us take a look at the statistics of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "##Read excel spread sheet of data\n",
    "data = pd.read_excel('cleaned_data.xls', header=0)\n",
    "\n",
    "##Declare the column names\n",
    "data.columns = ['date', 's_and_p_comp', 'dividend', 'earnings',\n",
    "                'CPI', 'fraction_date', 'long_interest_rate', 'real_price',\n",
    "                'real_dividend', 'real_total_return_price','real_earnings',\n",
    "                'real_scaled_earnings', 'CAPE', 'TR_CAPE', 'excess_CAPE', 'montly_bond_return',\n",
    "                'real_bond_return','10_year_stock_return', '10_year_bond_return',\n",
    "                '10_year_excess_return']\n",
    "\n",
    "\n",
    "for col in data.columns:\n",
    "    if is_numeric_dtype(data[col]):\n",
    "        print('%s:' % (col))\n",
    "        print('\\t Mean = %.2f' % data[col].mean())\n",
    "        print('\\t Standard deviation = %.2f' % data[col].std())\n",
    "        print('\\t Minimum = %.2f' % data[col].min())\n",
    "        print('\\t Maximum = %.2f' % data[col].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is visible, that the 's_and_p_comp', 'real_price', 'real_total_return_price' and 'real_scaled_earnings' have very high standard deviations, which could be a sign, that they are accumulations or at least grow over time. \n",
    "***\n",
    "The covariance was not as insightful, but the correlation is interesting. We see that certain rows have a very high correlation. This is due to the corresponding calcuations that are performed on the first rows. For example the 'real_price', 'real_dividend' and the 'real_total_return_price' are almost completly correlated. This is due to the calculation of the 'real_total_return_price', which depends of the other to attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Correlation:')\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Now we take a look at the joint distribution. Before that, we remove the 'date' and 'fraction_date' to get graphs, which show the distribution between two attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_jd = data\n",
    "\n",
    "data_jd = data_jd.drop(['date'], axis=1)\n",
    "data_jd = data_jd.drop(['fraction_date'], axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12,12))\n",
    "index = 0\n",
    "for i in range(3):\n",
    "    for j in range(i+1,4):\n",
    "        ax1 = int(index/2)\n",
    "        ax2 = index % 2\n",
    "        axes[ax1][ax2].scatter(data_jd[data_jd.columns[i]], data_jd[data_jd.columns[j]], color='red')\n",
    "        axes[ax1][ax2].set_xlabel(data_jd.columns[i])\n",
    "        axes[ax1][ax2].set_ylabel(data_jd.columns[j])\n",
    "        index = index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All graphs have a high correlation, which we can see due to similar reactions to higher values. This was already shown in the correlation analysis. 'CPI' and 's_and_p_comp' have a small disbalance in direction to 'CPI' ('CPI' grows faster, with higher values). \n",
    "\n",
    "This could be useful for handling redundancy, as redundant attributes can be detected through correlation and covariance. If we think about the sentence from the beginning, this makes totally sense, as most attributes are calculated with the first four columns.\n",
    "\n",
    "It can be interesting, to analyse the anomalies in the graphs, or know the dates, since they are possible financial recessions or growth periods. \n",
    "\n",
    "***\n",
    "## Preprocessing\n",
    "\n",
    "\n",
    "### Data Quality Issues\n",
    "First, we read the excel-file and define the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "##Read excel spread sheet of data\n",
    "data = pd.read_excel('cleaned_data.xls', header=0)\n",
    "\n",
    "##Declare the column names\n",
    "data.columns = ['date', 's_and_p_comp', 'dividend', 'earnings',\n",
    "                'CPI', 'fraction_date', 'long_interest_rate', 'real_price',\n",
    "                'real_dividend', 'real_total_return_price','real_earnings',\n",
    "                'real_scaled_earnings', 'CAPE', 'TR_CAPE', 'excess_CAPE', 'montly_bond_return',\n",
    "                'real_bond_return','10_year_stock_return', '10_year_bond_return',\n",
    "                '10_year_excess_return']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace all 'NA' rows with NAN type. \n",
    "Then we drop the '10_year_stock_return', '10_year_bond_return' and '10_year_excess_return', as the last rows do not yet exist and we decide to perform our analysis with the rest of the data. \n",
    "In the data were no missing values, which simplified our preprocessing step considerably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replaces all 'NA' rows with NAN type\n",
    "data = data.replace('NA',np.NaN)\n",
    "\n",
    "data.head()\n",
    "\n",
    "##Drop \"10 year\" columns so there are no rows with missing data after 2011   \n",
    "data = data.drop(['10_year_stock_return'],axis=1)\n",
    "data = data.drop(['10_year_bond_return'],axis=1)\n",
    "data = data.drop(['10_year_excess_return'],axis=1)\n",
    "\n",
    "##Drop rows with missing data\n",
    "print('\\n\\nNumber of rows in original data = %d' % (data.shape[0]))\n",
    "data = data.dropna()\n",
    "print('Number of rows after discarding missing values = %d\\n' % (data.shape[0]))\n",
    "\n",
    "#Number\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d\\n' % (data.shape[1]))\n",
    "\n",
    "##Check to make sure there are no missing values in each column\n",
    "print('Number of missing values:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col,data[col].isna().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "\n",
    "We took a look at possible outliers. The inital step was to exclude the 'date, 'fraction_date'. We also decide to exclude the 'real_total_return_price', 'real_scaled_earnings', 'earnings' and 'real_earnings' as it is an accumulation. The 's_and_p_comp', 'real_price', 'real_bond_return', 'dividend', 'real_dividend' and 'CPI' are also variables which increase over time, so they were excluded too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data_bp = data\n",
    "\n",
    "data_bp = data_bp.drop(['date'], axis=1)\n",
    "data_bp = data_bp.drop(['fraction_date'], axis=1)\n",
    "\n",
    "data_bp = data_bp.drop(['real_total_return_price'], axis=1)\n",
    "data_bp = data_bp.drop(['real_scaled_earnings'], axis=1)\n",
    "data_bp = data_bp.drop(['earnings'], axis=1)\n",
    "data_bp = data_bp.drop(['real_earnings'], axis=1)\n",
    "\n",
    "data_bp = data_bp.drop(['s_and_p_comp'], axis=1)\n",
    "data_bp = data_bp.drop(['real_price'], axis=1)\n",
    "data_bp = data_bp.drop(['real_bond_return'], axis=1)\n",
    "data_bp = data_bp.drop(['dividend'], axis=1)\n",
    "data_bp = data_bp.drop(['real_dividend'], axis=1)\n",
    "data_bp = data_bp.drop(['CPI'], axis=1)\n",
    "\n",
    "for row in data_bp.columns:\n",
    "    data_bp[row] = pd.to_numeric(data_bp[row])\n",
    "        \n",
    "data_bp.boxplot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'CAPE and 'TR_CAPE' are ratios, which are infaltion-adjusted. So there should be outliers. In the last years  (>2000) they grew stronger than in the years before, which explains the points higher than the maximum of the boxplot. The same reasoning occurs to the long_interest_rate.\n",
    "\n",
    "In total, the box-plot is not as interesting, because we have high volatility and the min. and max. values are set to narrow.\n",
    "\n",
    "*** \n",
    "\n",
    "If we look only at the 'montly_bond_return' before 1921, we do not have outliers, but after that we do. Especially the bull stock market starting from 1922 highly increased the 'montly_bond_return' and resulted in the crash from 1929. At the start of the Second World War, the 'montly_bond_return' had strong losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bp2 = data[(data['date'] < '1945')]\n",
    "\n",
    "data_bp2 = data_bp2.drop(['date'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['fraction_date'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['real_total_return_price'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['real_scaled_earnings'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['earnings'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['real_earnings'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['s_and_p_comp'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['real_price'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['real_bond_return'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['dividend'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['real_dividend'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['CPI'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['CAPE'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['TR_CAPE'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['long_interest_rate'], axis=1)\n",
    "data_bp2 = data_bp2.drop(['excess_CAPE'], axis=1)\n",
    "\n",
    "for row in data_bp2.columns:\n",
    "    data_bp2[row] = pd.to_numeric(data_bp2[row])\n",
    "        \n",
    "data_bp2.boxplot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data it does not make sense to exclude outliers, as they give us the most interesting insights.\n",
    "***\n",
    "## Aggregation\n",
    "\n",
    "Here we take a look at the graphic visualization of the data for the 's_and_p_comp'. The same can be done with other columns. We see that the highest increases occured after the 1990s, under the regency of Ronald Reagan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = data\n",
    "daily.index = pd.to_datetime(daily['date'])\n",
    "\n",
    "daily = daily['s_and_p_comp']\n",
    "ax = daily.plot(kind='line',figsize=(20,3))\n",
    "ax.set_title('Daily Precipitation (variance = %.4f)' % (daily.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = daily.groupby(pd.Grouper(freq='M')).sum()\n",
    "annual = monthly.groupby(pd.Grouper(freq='Y')).sum()\n",
    "ax = annual.plot(kind='line',figsize=(20,3))\n",
    "ax.set_title('Annual Precipitation (variance = %.4f)' % (annual.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "If we take a look at the CAPE ratio, we see the biggest increases and fallbacks between 1922-29, at the dot-com bubble in the 2000s and the financial crisis from 2009. \n",
    "\n",
    "Other events are not as visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = data\n",
    "daily.index = pd.to_datetime(daily['date'])\n",
    "\n",
    "daily = daily['CAPE']\n",
    "ax = daily.plot(kind='line',figsize=(20,3))\n",
    "ax.set_title('Daily Precipitation (variance = %.4f)' % (daily.var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "For our data set, random sampling does not make sense since we are looking at fincial change over *time*. Instead, our sample is a random 10 conescutive rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "randomIndex = random.randint(0,data.shape[0])\n",
    "sample = data[randomIndex:randomIndex+10]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "\n",
    "Here we transform the continuous attribute 'real_price' in categories. We can already see, that the highest occurence is for a 'real_price' below 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['real_price'].hist(bins=10)\n",
    "data['real_price'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "If we order it into bins this can be also seen, which gives similar insights as the graph before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(data['real_price'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.qcut(data['real_price'],4)\n",
    "bins.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Classification\n",
    "In the preprocessing we already modified our data so we can continue with the classification.\n",
    "For the financial data we took the Decision Tree Induction based on the interest rate. The criteria for a partition is the entropy. \n",
    "\n",
    "\n",
    "## Decision Tree\n",
    "We start with the model construction to define classes and put each tuple eventually in a class. At the beginning all entities belong to the root. Then they are recursively partitioned in classes.\n",
    "\n",
    "In the end we want to have a decision tree to classify future data, for example when the excel-sheet is updated.\n",
    "\n",
    "### Encoding in a Binary Format\n",
    "For this tree, we are going to format long_interest_rate column as an example. We have defined an interest rate > 7 to be high, and everything < 7 to be low. It is visible in our data that an interest rate higher than 7% rarely occurs.\n",
    "\n",
    "For this example, -1 == Low and 1 == High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterLow = data['long_interest_rate'] >= 7\n",
    "filterHigh = data['long_interest_rate'] < 7\n",
    "\n",
    "data['long_interest_rate'].where(filterLow, -1, inplace = True)\n",
    "data['long_interest_rate'].where(filterHigh, 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the decision tree only determines if long_interest_rate is low or high.\n",
    "### Creating the Tree\n",
    "To create this tree, we go with a depth of 4 because past that, it was apparent that we ran into over-fitting. The classes only had low numbers of samples and the tree got unnecassary complex. \n",
    "\n",
    "The classes are based on the low/high of long_interest_rate since that is the obvious filter of upward/downard trending years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "Y = data['long_interest_rate']\n",
    "X = data.drop(['date','long_interest_rate'],axis=1)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',max_depth=4)\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus \n",
    "from IPython.display import Image\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, feature_names=X.columns, class_names=['Low','High'], filled=True, \n",
    "                                out_file=None) \n",
    "graph = pydotplus.graph_from_dot_data(dot_data) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the entropy is overall relatively low. Especially, for the red coloured leafs do we have an entropy of 0, which means there exists no uncertainty in regard to the class.\n",
    "\n",
    "In a next step we look at the information gain of the different columns and determine the best suited attribute. We would do this with the Information Gain, Gini Index and Gain Ratio. Since 15 columns are calculations from the first 4 columns, there should not be huge differences, but we could take a look at the first columns and decide between 's_and_p_comp', 'dividend', 'earnings' and 'CPI'.\n",
    "\n",
    "***\n",
    "\n",
    "### Accuracy\n",
    "To test the accuracy for the model usage, we run the following code. We can see that the prediction has a 99%, which is almost optimal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predY = clf.predict(X)\n",
    "predictions = pd.concat([data['date'],pd.Series(predY,name='Predicted Class')], axis=1)\n",
    "print('Accuracy on test data is %.2f' % (accuracy_score(Y, predY)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Clustering\n",
    "\n",
    "Here we tried to find similarities between entities. As we already know when crisis or recessions happened, we can compare if our result make sense. \n",
    "\n",
    "## K-means Clustering\n",
    "\n",
    "We start with the K-means Clustering. The sensitivity of that approach is for our use-case quiet helpfull, as we want to detect anomalies of the stock market during certain time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recession_data = data[(data['date'] > '2006') & (data['date'] < '2011')]\n",
    "recession_data = recession_data[['date', 'real_price', 'real_scaled_earnings']]\n",
    "recession_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These columns are taken from the data from 2006-2010. This is an interesting time period, because part of it is during the Great Recession. The data will be put into three clusters, which hopefully ends up in clusters before the financial crisis, during and after the crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "clustering_data = recession_data.drop('date',axis=1)\n",
    "k_means = cluster.KMeans(n_clusters=3, max_iter=50, random_state=1)\n",
    "k_means.fit(clustering_data) \n",
    "labels = k_means.labels_\n",
    "pd.DataFrame(labels, index=recession_data.date, columns=['Cluster ID'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The official time of the Great Recession is December 2007 - June 2009. The clusters don't exactly line up with this but are instead delayed several months. This is because it took time for the recession and end of the recession to affect the selected metrics. The 'real_price' and 'real_scaled_earnings' are attributes, which react slower than for example the interest rate.\n",
    "\n",
    "The k-means clustering algorithm placed the non-recession dates into Cluster 1. This is the time of stable behavior and low volatility.\n",
    "\n",
    "The dates in the transtion time into and out of the recession were placed into Cluster 2. In that timespan are  changes already visible.\n",
    "\n",
    "The dates that are within the recession were placed in Cluster 0. Here we have a strong reduction of both metrics and high volatility. \n",
    "\n",
    "***\n",
    "\n",
    "# Hierarchical Clustering\n",
    "\n",
    "Now we use the single link hierarchical clustering. The results did not differ with the complete or average link clustering. The single link method clusters entities with the smallest distance together.\n",
    "As we took only parts of the data set, the disadvantage of the hierarchical clustering with large datasets did not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_1940s = data[(data['date'] > '1940') & (data['date'] < '1950')]\n",
    "data_1940s = data_1940s[data_1940s['date'].str.endswith('01')]\n",
    "data_1940s = data_1940s[['date', 'real_price', 'real_dividend', 'real_total_return_price', 'real_scaled_earnings']]\n",
    "data_1940s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data represents the beginning of each year in the 1940s. World War 2 went until 1945 so the earlier years of the 1940s are expected to be connected closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dates = data_1940s['date']\n",
    "data_matrix = data_1940s.drop(['date'],axis=1)\n",
    "linkage = hierarchy.linkage(data_matrix.values, 'single')\n",
    "dn = hierarchy.dendrogram(linkage,labels=dates.tolist(),orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1940s are closely connected, as expected. 1940 and 1944, which are respectively close to the start and end of WW2, are very closely connected. \n",
    "\n",
    "The begin of year 1941 and 1943 seem to have similar trends. The same happens for the years 1940 and 1944, which are near the begin and end of the second world war. After the war the years 1945 and 1947 are related, where the big upwind for the US economy starts. 1946 is a large outlier, seen by a different colouring, which may be due to the strong rebound after the war. \n",
    "\n",
    "***\n",
    "\n",
    "# Summary and Evaluation\n",
    "\n",
    "Our data was already well cleaned so the preprocessing did not perform many changes to the data. The upfront data exploration helped to get an understanding of the columns and which attributes grew over time (ex. 'real_earnings) and which kept a similar level ('dividend', 'CAPE'). Due to that we could focus our analysis on certain parts and adjust the preprocessing, where we for example dropped the 10 year return columns (they were not complete and gave no new insights). \n",
    "\n",
    "We thought about doing the frequent pattern anaylsis, but decided against it, since the learnt methods did not fit to our data (no market baskets or sets). \n",
    "\n",
    "The classification gave us a good sense, which entities belong together and acts as a tool for future updates of the data. We think the decision tree was here very accurate.\n",
    "\n",
    "The clustering offered us a lot of possibilities, as there were many turbulences in the past financial universe. The K-Means Clustering, in contrast to the hierarchical clustering, gave better results. We could cleary identify the start and end of the 2009 financial crisis and seperate it from the stable periods. The crisis itself too, but that was expected. The hierarchical clustering put similar periods together, but a clear hierarchy is not visible. That is either due to used method or the analysed timeperiod. \n",
    "\n",
    "In total, we are satisfied with our results. A next step would be to take a deeper look in the frequent pattern analysis and to find a matching method to our data. For our project is the classification and clustering analysis sufficient, since they already gave good insights.\n",
    "***\n",
    "Thanks for the good time in the lectures and the interesting topics!\n",
    "\n",
    "Sincerely,\n",
    "\n",
    "Brady & Cole & Lukas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
